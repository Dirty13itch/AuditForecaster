# Incident Response Playbook
## Energy Auditing Platform - Production Incident Management

**Last Updated:** October 30, 2025  
**Owner:** Platform Team  
**Emergency Contact:** ops@energyaudit.example.com  
**Slack Channel:** #incidents

---

## Table of Contents

1. [Overview](#overview)
2. [Incident Severity Levels](#incident-severity-levels)
3. [Incident Response Workflow](#incident-response-workflow)
4. [Common Incident Scenarios](#common-incident-scenarios)
5. [Communication Templates](#communication-templates)
6. [On-Call Schedule & Escalation](#on-call-schedule--escalation)
7. [Post-Incident Procedures](#post-incident-procedures)
8. [Appendix](#appendix)

---

## Overview

This playbook provides standardized procedures for detecting, responding to, and resolving production incidents in the Energy Auditing Platform.

### Goals

- **Minimize Impact**: Reduce user-facing downtime and data loss
- **Clear Communication**: Keep stakeholders informed throughout incidents
- **Fast Resolution**: Systematic approach to identify and fix root causes
- **Continuous Improvement**: Learn from incidents to prevent recurrence

### Related Documentation

- [Monitoring Runbook](./MONITORING_RUNBOOK.md) - Alert response procedures
- [Backup & Restore Guide](./BACKUP_RESTORE_GUIDE.md) - Data recovery procedures
- [API Documentation](./api/index.html) - System architecture reference

---

## Incident Severity Levels

### SEV-1: Critical
**üî¥ Immediate Response Required**

**Definition:**
- Complete application outage (unavailable to all users)
- Critical data loss or corruption
- Security breach or data exposure
- Payment processing failure affecting revenue

**Response Time:** Immediate (< 5 minutes)

**Escalation:**
- Page on-call engineer immediately
- Notify engineering manager
- Create war room in Slack #incidents
- Consider paging CEO/CTO for major outages

**Examples:**
- Database completely unreachable
- Application returns 500 errors to all users
- Photo storage system failure preventing uploads
- Authentication system down (no one can log in)
- Data breach detected

---

### SEV-2: High
**üü° Urgent Response Required**

**Definition:**
- Major feature completely degraded
- Elevated error rates (> 5% of requests)
- Database connection pool exhaustion
- Critical API endpoints failing
- Performance degradation affecting > 25% of users

**Response Time:** 15 minutes

**Escalation:**
- Alert on-call engineer via Slack
- Notify engineering lead
- Post in #incidents channel
- Update status page if user-facing

**Examples:**
- Photo upload failing for some users
- Job creation endpoint timing out
- Report generation extremely slow (> 30s)
- Google Calendar sync failures
- Email notifications not sending

---

### SEV-3: Medium
**üîµ Standard Response**

**Definition:**
- Minor feature issues
- Slow performance (not critical)
- Non-critical bugs
- Errors affecting < 5% of requests
- Single user or small group impacted

**Response Time:** 1 hour

**Escalation:**
- Email on-call engineer
- Post in #eng-support channel
- Create JIRA ticket
- No immediate status page update

**Examples:**
- Photo thumbnail generation slow
- Minor UI rendering issues
- Non-critical report field missing
- Calendar event color not syncing
- Minor validation error in edge case

---

### SEV-4: Low
**‚ö™ Low Priority**

**Definition:**
- Cosmetic issues
- Enhancement requests
- Documentation updates
- Low-priority bugs
- Feature requests

**Response Time:** Next business day

**Escalation:**
- Add to backlog
- Discuss at next planning meeting
- No immediate action required

**Examples:**
- Typos in UI text
- Minor styling inconsistencies
- Feature request for new report type
- Documentation improvements
- Non-critical performance optimizations

---

## Incident Response Workflow

### Phase 1: Detection (0-5 minutes)

**Sources of Detection:**

1. **Automated Monitoring**
   - Prometheus alerts (see [Monitoring Runbook](./MONITORING_RUNBOOK.md))
   - Sentry error spike notifications
   - Health check failures
   - Performance degradation alerts

2. **User Reports**
   - Support tickets
   - Slack messages
   - Email notifications
   - Phone calls from field inspectors

3. **Manual Discovery**
   - Team member notices issue
   - Smoke test failures
   - Deployment verification fails

**Immediate Actions:**

```bash
# Check service health
curl https://your-domain.repl.co/healthz
curl https://your-domain.repl.co/api/status

# Check Grafana dashboards
open https://grafana.example.com/d/energy-audit-overview

# Check Sentry for error spikes
open https://sentry.io/organizations/your-org/issues/

# Check application logs
tail -f logs/error.log
```

---

### Phase 2: Triage (5-15 minutes)

**Assess Severity:**

Use this decision tree:

1. **Are all users affected?** ‚Üí SEV-1
2. **Is critical functionality down?** ‚Üí SEV-1
3. **Are > 25% of users affected?** ‚Üí SEV-2
4. **Is error rate > 5%?** ‚Üí SEV-2
5. **Is performance severely degraded?** ‚Üí SEV-2
6. **Are < 5% of users affected?** ‚Üí SEV-3
7. **Is this cosmetic or minor?** ‚Üí SEV-4

**Create Incident Ticket:**

```markdown
Title: [SEV-X] Brief description of issue
Severity: SEV-X
Status: Investigating
Started: 2025-10-30 14:30 UTC
Detected By: [Prometheus Alert | User Report | Manual]
Affected Systems: [API | Database | Object Storage | Auth]
Impact: [Number of users | Features affected]
Incident Lead: @engineer-name
```

**Identify Scope:**

- How many users are affected?
- Which features are impacted?
- Is this getting worse?
- When did it start?
- What changed recently?

---

### Phase 3: Response (15-60 minutes)

**Immediate Mitigation:**

For each severity level:

**SEV-1 Response:**
1. Stop the bleeding (rollback, disable feature, failover)
2. Communicate incident start to all stakeholders
3. Create war room in Slack #incidents
4. Assign incident commander
5. Page all hands if needed

**SEV-2 Response:**
1. Identify affected component
2. Review recent deployments/changes
3. Check related alerts and logs
4. Implement quick fix or rollback
5. Monitor metrics for improvement

**SEV-3/4 Response:**
1. Create ticket with reproduction steps
2. Assign to appropriate team member
3. Schedule fix for next sprint
4. Document workaround if available

**Common Actions:**

```bash
# Rollback recent deployment
git revert HEAD
git push origin main

# Restart application (Replit auto-restarts)
# Kill process to force restart
pkill -f "node server"

# Scale database connections
# Update DATABASE_URL connection pool size

# Check external service status
curl https://status.neon.tech/
curl https://status.cloud.google.com/
curl https://status.sendgrid.com/

# Review recent changes
git log --oneline -n 10
```

---

### Phase 4: Resolution (60-120 minutes)

**Verify Fix Deployed:**

```bash
# Deploy verification
npm run test:smoke

# Check key metrics
curl https://your-domain.repl.co/api/status

# Verify in Grafana
# - Error rate back to < 1%
# - Latency back to normal
# - No new alerts firing
```

**Confirm Normal Operation:**

- Error rate < 1%
- Latency p95 < 500ms
- No active alerts
- User reports stopped
- Smoke tests passing

**Close Incident:**

1. Update incident ticket to "Resolved"
2. Post resolution message (see templates)
3. Thank responders
4. Schedule post-mortem (if SEV-1 or SEV-2)

---

### Phase 5: Post-Mortem (Within 48 hours for SEV-1/2)

**Post-Mortem Template:**

```markdown
# Incident Post-Mortem: [Brief Description]

**Date:** 2025-10-30
**Severity:** SEV-X
**Duration:** X hours Y minutes
**Impact:** X users affected, Y feature degraded
**Incident Lead:** @engineer-name

## Timeline

- **14:30 UTC** - Alert fired: HighErrorRate
- **14:32 UTC** - On-call engineer paged
- **14:35 UTC** - Incident created, war room started
- **14:40 UTC** - Root cause identified: Database connection pool exhaustion
- **14:45 UTC** - Mitigation deployed: Increased connection pool size
- **15:00 UTC** - Metrics returned to normal
- **15:15 UTC** - Incident resolved

## Root Cause

[Detailed technical explanation of what went wrong]

## What Went Well

- Fast detection via Prometheus alerts
- Clear escalation path followed
- Effective communication in war room
- Quick mitigation deployed

## What Could Be Improved

- Better connection pool monitoring needed
- Load testing didn't catch this scenario
- Documentation could be clearer on scaling

## Action Items

- [ ] Add Prometheus alert for connection pool usage (@engineer1, due: Nov 5)
- [ ] Update load testing suite to include concurrent connections (@engineer2, due: Nov 10)
- [ ] Document connection pool scaling in runbook (@engineer3, due: Nov 7)
- [ ] Implement connection pool auto-scaling (@engineer4, due: Nov 15)

## Prevention

- Connection pool monitoring added
- Auto-scaling configured
- Load test updated
- Runbook updated with scaling procedures
```

---

## Common Incident Scenarios

### Scenario 1: High Error Rate Alert

**Alert:** `HighErrorRate` from Prometheus

**Symptoms:**
- HTTP 5xx error rate > 1%
- Sentry showing error spike
- Users reporting failures

**Runbook Reference:** [Monitoring Runbook - High Error Rate](./MONITORING_RUNBOOK.md#high-error-rate)

**Quick Actions:**

```bash
# 1. Check Grafana for failing endpoints
open https://grafana.example.com/d/api-overview

# 2. Check error logs
grep "500\|502\|503" logs/combined.log | tail -100

# 3. Check Sentry for stack traces
open https://sentry.io/organizations/your-org/issues/

# 4. Review recent deployments
git log --oneline -n 5

# 5. Rollback if needed
git revert HEAD && git push
```

**Common Root Causes:**
- Recent code deployment introduced bug
- Database query timeout
- External API failure (Google Calendar, SendGrid)
- Memory leak causing crashes

**Communication:**
```
üö® SEV-2: High error rate detected on /api/jobs endpoint
Impact: Job creation failing for ~10% of users
Status: Investigating - checking recent deployments
Lead: @engineer-name
```

---

### Scenario 2: Database Connection Failures

**Alert:** `DatabaseConnectionFailures` from Prometheus

**Symptoms:**
- Application can't connect to database
- All database queries failing
- 500 errors on all endpoints

**Severity:** SEV-1 (Complete outage)

**Quick Actions:**

```bash
# 1. Check Neon status
curl https://status.neon.tech/api/v2/status.json

# 2. Test database connection
psql $DATABASE_URL -c "SELECT 1"

# 3. Check connection pool
# Review logs for "too many connections"
grep "too many connections" logs/error.log

# 4. Verify DATABASE_URL is correct
echo $DATABASE_URL | grep -o "postgresql://[^:]*"

# 5. Restart application
pkill -f "node server"
```

**Escalation Path:**
1. Check Neon status page
2. If Neon is down: Wait for recovery, communicate to users
3. If credentials changed: Update DATABASE_URL in Secrets
4. If connection pool exhausted: Scale up pool size
5. If persistent: Contact Neon support

**Recovery Procedure:**

```bash
# Option 1: Wait for Neon recovery (if Neon incident)
# Monitor status.neon.tech

# Option 2: Failover to backup (if configured)
export DATABASE_URL=$BACKUP_DATABASE_URL
# Restart application

# Option 3: Restore from backup
# See BACKUP_RESTORE_GUIDE.md
```

**Communication:**
```
üö® SEV-1: Database connection failure - complete outage
Impact: All users cannot access application
Status: Checking Neon status - possible provider incident
Lead: @engineer-name
ETA: Monitoring Neon for resolution
```

---

### Scenario 3: Object Storage Failures

**Alert:** Photo uploads failing, GCS errors in logs

**Symptoms:**
- Photo upload returns 500 error
- Thumbnail generation failing
- GCS permission errors in logs

**Severity:** SEV-2 (Major feature degraded)

**Quick Actions:**

```bash
# 1. Check GCS status
curl https://status.cloud.google.com/

# 2. Test upload manually
echo "test" > /tmp/test.txt
gsutil cp /tmp/test.txt gs://repl-default-bucket-${REPL_ID}/test/

# 3. Check bucket permissions
gsutil iam get gs://repl-default-bucket-${REPL_ID}

# 4. Verify service account key
ls -la $GCS_KEY_FILE_PATH

# 5. Check quota
gcloud compute project-info describe --project=$GCS_PROJECT_ID
```

**Common Root Causes:**
- GCS service outage
- Service account key expired/revoked
- Bucket permissions changed
- Storage quota exceeded
- Network connectivity issues

**Mitigation Options:**

```bash
# Option 1: Rotate service account key
# Generate new key in GCP Console
# Update GCS_KEY_FILE_PATH in Secrets

# Option 2: Temporarily disable feature
# Add feature flag to disable uploads
# Show maintenance message to users

# Option 3: Failover to backup bucket
export BUCKET_NAME=$BACKUP_BUCKET_NAME
# Restart application
```

**Communication:**
```
üü° SEV-2: Photo upload failures
Impact: Users cannot upload new photos
Status: Investigating GCS connectivity
Workaround: Other features functioning normally
Lead: @engineer-name
```

---

### Scenario 4: Authentication System Down

**Alert:** Users reporting "Cannot log in"

**Symptoms:**
- OIDC callback failures
- Session creation errors
- Redirect loop on login

**Severity:** SEV-1 (Cannot access application)

**Quick Actions:**

```bash
# 1. Check Replit Auth status
curl https://replit.com/auth/status

# 2. Test OIDC flow
curl https://your-domain.repl.co/api/health

# 3. Check registered domains
echo $REPLIT_DOMAINS

# 4. Verify OIDC configuration
curl https://auth.replit.com/.well-known/openid-configuration

# 5. Check session store
psql $DATABASE_URL -c "SELECT COUNT(*) FROM sessions"
```

**Diagnostic Commands:**

```bash
# Get auth diagnostics (admin only)
curl -b cookies.txt https://your-domain.repl.co/api/auth/diagnostics

# Check recent auth errors
tail -100 logs/auth-errors.log
```

**Common Root Causes:**
- Domain not registered in REPLIT_DOMAINS
- OIDC redirect URI mismatch
- Session store failure (database down)
- SESSION_SECRET changed/missing
- Replit Auth service incident

**Recovery Procedure:**

See [Auth Troubleshooting Guide](./api/auth-troubleshooting.md)

**Communication:**
```
üö® SEV-1: Authentication system failure
Impact: All users cannot log in
Status: Investigating OIDC configuration
Lead: @engineer-name
Updates: Every 15 minutes in #incidents
```

---

### Scenario 5: Performance Degradation

**Alert:** `HighAPILatency` - P95 latency > 500ms

**Symptoms:**
- Slow page loads
- Request timeouts
- Users reporting "app is slow"

**Severity:** SEV-2 to SEV-3 depending on impact

**Quick Actions:**

```bash
# 1. Check Grafana latency dashboard
open https://grafana.example.com/d/api-latency

# 2. Identify slow endpoints
grep "duration" logs/combined.log | sort -k4 -rn | head -20

# 3. Check database query performance
psql $DATABASE_URL -c "SELECT * FROM pg_stat_statements ORDER BY mean_exec_time DESC LIMIT 10"

# 4. Check memory usage
free -h

# 5. Check CPU usage
top -bn1 | head -20
```

**Common Root Causes:**
- Unoptimized database query (missing index)
- N+1 query problem
- Memory leak
- High concurrent load
- External API slow response

**Investigation Steps:**

```bash
# Profile slow endpoint
curl -w "@curl-format.txt" https://your-domain.repl.co/api/jobs

# Check database slow queries
tail -100 logs/db-slow-queries.log

# Check for memory leaks
node --inspect server/index.ts
# Open chrome://inspect and take heap snapshot
```

**Mitigation:**

```bash
# Option 1: Add database index
psql $DATABASE_URL -c "CREATE INDEX idx_jobs_scheduled_date ON jobs(scheduled_date)"

# Option 2: Add caching
# Implement Redis/in-memory cache for frequent queries

# Option 3: Increase resources
# Scale up Repl resources if available

# Option 4: Rate limit heavy endpoints
# Add rate limiting to prevent abuse
```

**Communication:**
```
üü° SEV-2: Slow performance on job listing
Impact: Users experiencing 2-3 second delays
Status: Identified missing database index
Fix: Deploying index, ETA 5 minutes
Lead: @engineer-name
```

---

### Scenario 6: Email Notifications Not Sending

**Alert:** User reports not receiving notifications

**Symptoms:**
- SendGrid errors in logs
- Email queue backing up
- No emails sent in last hour

**Severity:** SEV-3 (Non-critical feature)

**Quick Actions:**

```bash
# 1. Check SendGrid status
curl https://status.sendgrid.com/api/v2/status.json

# 2. Verify API key
# Check that SENDGRID_API_KEY is set
echo $SENDGRID_API_KEY | cut -c1-20

# 3. Check SendGrid dashboard
open https://app.sendgrid.com/

# 4. Test email send
curl -X POST https://api.sendgrid.com/v3/mail/send \
  -H "Authorization: Bearer $SENDGRID_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{"personalizations":[{"to":[{"email":"test@example.com"}]}],"from":{"email":"noreply@energyaudit.com"},"subject":"Test","content":[{"type":"text/plain","value":"Test"}]}'

# 5. Check email logs
grep "sendgrid" logs/combined.log | tail -50
```

**Common Root Causes:**
- SendGrid API key expired/revoked
- Email quota exceeded
- From address not verified
- Blocked/spam listed
- SendGrid service outage

**Recovery:**

```bash
# Option 1: Regenerate API key
# Create new key in SendGrid console
# Update SENDGRID_API_KEY in Secrets

# Option 2: Queue emails for retry
# Emails will be retried automatically

# Option 3: Use alternative notification
# Send SMS via Twilio
# In-app notifications
```

**Communication:**
```
üîµ SEV-3: Email notifications delayed
Impact: Job assignment emails not sending
Status: SendGrid API key issue identified
Fix: Regenerating API key, emails will be retried
Lead: @engineer-name
```

---

## Communication Templates

### Incident Start

**For Slack (#incidents):**

```
üö® INCIDENT STARTED

**Severity:** SEV-X
**Title:** [Brief description]
**Impact:** [What users are experiencing]
**Status:** Investigating
**Started:** [Timestamp UTC]
**Lead:** @engineer-name

**Affected Systems:** [API | Database | Storage | Auth]
**User Impact:** [X% of users | All users | Specific feature]

**Updates:** Will post every 15 minutes in this thread
**Dashboard:** [Grafana link]
**Sentry:** [Sentry issue link]
```

**For Status Page:**

```
‚ö†Ô∏è Investigating: [Service Name]

We are currently investigating reports of [issue description]. 
Some users may be experiencing [specific impact].

Updates will be posted as we learn more.

Posted: [Timestamp]
```

---

### Incident Update

**Every 15 minutes during SEV-1/2:**

```
üìä UPDATE [HH:MM UTC]

**Status:** [Investigating | Identified | Monitoring | Resolved]

**What we know:**
- [Key finding 1]
- [Key finding 2]

**What we're doing:**
- [Action 1]
- [Action 2]

**Next update:** [Timestamp or "when we know more"]
```

---

### Incident Resolution

**For Slack:**

```
‚úÖ INCIDENT RESOLVED

**Duration:** X hours Y minutes
**Root Cause:** [Brief technical summary]
**Fix:** [What was done]

**Metrics:**
- Error rate: Back to < 1%
- Latency: Back to normal
- All alerts: Cleared

**Next Steps:**
- Post-mortem scheduled for [date/time]
- Action items will be tracked in JIRA

**Thank you** to @engineer1, @engineer2 for quick response!
```

**For Status Page:**

```
‚úÖ Resolved: [Service Name]

This incident has been resolved. [Service] is now operating normally.

Root cause: [Brief explanation]

Thank you for your patience.

Resolved: [Timestamp]
```

---

### Post-Incident Summary (Email to Leadership)

```
Subject: [SEV-X] Incident Summary: [Brief Description]

Hi team,

We experienced a [SEV-X] incident today affecting [description].

**Impact:**
- Duration: X hours Y minutes
- Users affected: X users (X%)
- Features impacted: [List]
- Revenue impact: $X (if applicable)

**Timeline:**
- Started: [Timestamp]
- Detected: [Timestamp] (+X minutes)
- Mitigated: [Timestamp] (+X minutes)
- Resolved: [Timestamp] (+X minutes)

**Root Cause:**
[Technical explanation]

**Prevention:**
We are taking the following actions to prevent recurrence:
1. [Action item 1]
2. [Action item 2]
3. [Action item 3]

**Post-Mortem:**
Full post-mortem available at: [Link]

Let me know if you have questions.

Best,
[Incident Lead]
```

---

## On-Call Schedule & Escalation

### On-Call Rotation

**Primary On-Call:**
- Week of Oct 28 - Nov 3: @engineer1
- Week of Nov 4 - Nov 10: @engineer2
- Week of Nov 11 - Nov 17: @engineer3

**Secondary On-Call:**
- Week of Oct 28 - Nov 3: @engineer2
- Week of Nov 4 - Nov 10: @engineer3
- Week of Nov 11 - Nov 17: @engineer1

**Engineering Manager:** @manager-name

### Escalation Path

**SEV-1 Escalation:**
```
1. Primary on-call (immediate page)
   ‚îî‚îÄ No response in 5 min ‚Üí 
2. Secondary on-call (immediate page)
   ‚îî‚îÄ No response in 5 min ‚Üí
3. Engineering Manager (phone call)
   ‚îî‚îÄ No response in 10 min ‚Üí
4. VP Engineering (phone call)
```

**SEV-2 Escalation:**
```
1. Primary on-call (Slack alert)
   ‚îî‚îÄ No response in 15 min ‚Üí
2. Secondary on-call (Slack alert)
   ‚îî‚îÄ No response in 15 min ‚Üí
3. Engineering Manager (email + Slack)
```

**SEV-3/4 Escalation:**
```
1. Create ticket in backlog
2. Assign to appropriate team
3. Discuss at next standup
```

### Contact Methods

**Primary: PagerDuty**
- SEV-1: Phone call + SMS + push notification
- SEV-2: Push notification + SMS

**Secondary: Slack**
- Channel: #incidents
- Direct mention: @oncall-primary

**Tertiary: Email**
- oncall@energyaudit.example.com
- Forwarded to current on-call engineer

**Phone (Emergency Only):**
- Managed via PagerDuty
- Numbers not published here for security

---

## Post-Incident Procedures

### Immediate Post-Incident (Within 1 hour)

1. **Update all communication channels**
   - Mark incident as resolved in Slack
   - Update status page
   - Send summary email to leadership (SEV-1/2)

2. **Verify full recovery**
   - Run smoke test suite
   - Check all metrics back to normal
   - Verify no new related errors

3. **Thank the team**
   - Acknowledge responders
   - Highlight what went well
   - Celebrate the fix!

### Post-Mortem (Within 48 hours for SEV-1/2)

**Required for:** SEV-1, SEV-2  
**Optional for:** SEV-3 (if valuable learning)  
**Skip for:** SEV-4

**Process:**

1. **Schedule meeting** (within 48 hours)
   - 1 hour duration
   - Include all incident responders
   - Optional: Include affected stakeholders

2. **Prepare timeline**
   - Collect all timestamps
   - Gather logs, screenshots, metrics
   - Review Slack thread

3. **Write post-mortem** (use template above)
   - Focus on learning, not blame
   - Identify systemic issues
   - Define concrete action items

4. **Review meeting agenda**
   - Walk through timeline
   - Discuss root cause
   - Brainstorm prevention measures
   - Assign action items with owners & dates

5. **Follow up on action items**
   - Track in JIRA
   - Review progress weekly
   - Close action items when complete

### Runbook Updates

**After every incident:**
- Update relevant runbooks with new learnings
- Add new scenarios if encountered
- Update troubleshooting steps
- Improve documentation clarity

**Quarterly runbook review:**
- Review all incidents from last quarter
- Identify patterns and trends
- Update escalation contacts
- Refresh examples and screenshots

---

## Appendix

### Quick Reference Card

**Print this and keep at desk:**

```
==========================================
INCIDENT RESPONSE QUICK REFERENCE
==========================================

SEVERITY ASSESSMENT:
‚ñ° All users down ‚Üí SEV-1
‚ñ° Critical feature down ‚Üí SEV-1
‚ñ° Data loss ‚Üí SEV-1
‚ñ° Major feature degraded ‚Üí SEV-2
‚ñ° Error rate > 5% ‚Üí SEV-2
‚ñ° Minor issues ‚Üí SEV-3
‚ñ° Cosmetic ‚Üí SEV-4

FIRST ACTIONS:
1. Check /healthz and /api/status
2. Check Grafana dashboards
3. Check Sentry for errors
4. Review recent deployments
5. Create incident in Slack #incidents

ESCALATION:
SEV-1: Page immediately
SEV-2: Slack alert
SEV-3: Email
SEV-4: Backlog

CONTACTS:
Slack: #incidents
Email: oncall@energyaudit.example.com
Phone: Via PagerDuty

HELPFUL LINKS:
Grafana: https://grafana.example.com
Sentry: https://sentry.io/your-org
Neon Status: https://status.neon.tech
GCS Status: https://status.cloud.google.com

RUNBOOKS:
Monitoring: docs/MONITORING_RUNBOOK.md
Backup: docs/BACKUP_RESTORE_GUIDE.md
API Docs: docs/api/index.html
==========================================
```

### Communication Channels

| Channel | Purpose | Audience |
|---------|---------|----------|
| #incidents | Active incident coordination | Engineers, Managers |
| #eng-support | Non-urgent issues | Engineers |
| #ops-alerts | Automated alert feed | Engineering team |
| Status Page | User-facing updates | All users |
| Email (leadership) | Executive summaries | Leadership team |

### Related Tools

- **Prometheus:** Monitoring and alerting
- **Grafana:** Metrics visualization
- **Sentry:** Error tracking
- **PagerDuty:** On-call management
- **Slack:** Team communication
- **JIRA:** Incident tracking and action items
- **GitHub:** Code deployment and rollback

### Incident Metrics

**Track these metrics:**
- MTTD (Mean Time To Detect)
- MTTR (Mean Time To Resolve)
- Incidents per week/month
- SEV-1 frequency
- User-reported vs. automated detection ratio

**Review monthly:**
- Incident trends
- Common root causes
- Effectiveness of runbooks
- On-call rotation performance

---

**Document Version:** 1.0.0  
**Last Reviewed:** October 30, 2025  
**Next Review:** November 30, 2025  
**Maintained By:** Platform Team

---

## Feedback & Improvements

Have suggestions for improving this playbook?  
Post in #eng-runbooks or submit a PR to update this document.

**Recent Updates:**
- 2025-10-30: Initial version created
- Added common scenarios based on Phase 4 monitoring alerts
- Integrated with existing Monitoring Runbook
